{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad7b1b06",
   "metadata": {},
   "source": [
    "# 02 - Nettoyage et Préparation des Données\n",
    "\n",
    "**Objectif** : Produire un jeu de données propre, cohérent et exploitable pour l'analyse et le reporting.\n",
    "\n",
    "**Étapes** :\n",
    "- Traiter les valeurs manquantes (suppression, imputation et justification).\n",
    "- Corriger ou exclure les valeurs incohérentes.\n",
    "- Harmoniser les formats (dates, géographie, catégories).\n",
    "- Documenter les règles de transformation.\n",
    "\n",
    "**Livrables** : Dataset final nettoyé, scripts reproductibles, documentation synthétique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26197fde",
   "metadata": {},
   "source": [
    "## 1. Import des Librairies et Chargement des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33572566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import des librairies nécessaires\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Chemins des fichiers de données\n",
    "data_files = {\n",
    "    'demandes': '../data/demandes_service_public.csv',\n",
    "    'documents': '../data/documents_administratifs_ext.csv',\n",
    "    'centres': '../data/centres_service.csv',\n",
    "    'communes': '../data/details_communes.csv',\n",
    "    'socioeco': '../data/donnees_socioeconomiques.csv',\n",
    "    'logs': '../data/logs_activite.csv',\n",
    "    'reseau': '../data/reseau_routier_togo_ext.csv',\n",
    "    'developpement': '../data/developpement.csv'\n",
    "}\n",
    "\n",
    "# Chargement des datasets\n",
    "datasets = {}\n",
    "for name, path in data_files.items():\n",
    "    try:\n",
    "        datasets[name] = pd.read_csv(path, sep=',', encoding='utf-8')\n",
    "        print(f\"✅ {name} chargé : {datasets[name].shape[0]} lignes, {datasets[name].shape[1]} colonnes\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur chargement {name} : {e}\")\n",
    "\n",
    "# Aperçu rapide\n",
    "for name, df in datasets.items():\n",
    "    print(f\"\\n=== {name.upper()} ===\")\n",
    "    print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f8da74",
   "metadata": {},
   "source": [
    "## 2. Traitement des Valeurs Manquantes\n",
    "\n",
    "**Stratégie** :\n",
    "- Identifier les NaN par dataset.\n",
    "- Suppression si <5% et non critiques.\n",
    "- Imputation par moyenne/médiane/mode selon le type de variable.\n",
    "- Justification : Basé sur l'analyse EDA (ex. : médiane pour délais, mode pour catégories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour analyser et traiter les NaN\n",
    "def handle_missing_values(df, name):\n",
    "    print(f\"\\n=== Analyse NaN pour {name} ===\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    print(missing_pct[missing_pct > 0])\n",
    "\n",
    "    # Exemples de traitement (adapter selon EDA)\n",
    "    if name == 'demandes':\n",
    "        # Imputation médiane pour délais (distribution asymétrique)\n",
    "        if 'delai_effectif' in df.columns:\n",
    "            df['delai_effectif'].fillna(df['delai_effectif'].median(), inplace=True)\n",
    "            print(\"✅ Imputation médiane pour 'delai_effectif'\")\n",
    "        # Suppression si NaN critiques (ex. : ID)\n",
    "        df.dropna(subset=['id_demande'], inplace=True)\n",
    "        print(\"✅ Suppression des lignes sans 'id_demande'\")\n",
    "\n",
    "    elif name == 'centres':\n",
    "        # Mode pour catégories\n",
    "        df.fillna(df.mode().iloc[0], inplace=True)\n",
    "        print(\"✅ Imputation mode pour catégories\")\n",
    "\n",
    "    # Autres datasets : suppression si <1%\n",
    "    if missing_pct.sum() < 1:\n",
    "        df.dropna(inplace=True)\n",
    "        print(\"✅ Suppression des NaN (<1%)\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Appliquer à tous les datasets\n",
    "for name in datasets:\n",
    "    datasets[name] = handle_missing_values(datasets[name], name)\n",
    "\n",
    "print(\"\\n✅ Traitement NaN terminé pour tous les datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e47559",
   "metadata": {},
   "source": [
    "## 3. Correction des Valeurs Incohérentes\n",
    "\n",
    "**Stratégie** :\n",
    "- Détecter outliers via IQR (comme en EDA).\n",
    "- Corriger ou exclure (ex. : winsorisation pour capacités extrêmes).\n",
    "- Vérifier cohérence logique (ex. : âge >0, dates plausibles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73168491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour détecter et corriger outliers (IQR)\n",
    "def detect_outliers_iqr(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Fonction pour corriger incohérences\n",
    "def correct_inconsistencies(df, name):\n",
    "    print(f\"\\n=== Correction incohérences pour {name} ===\")\n",
    "\n",
    "    if name == 'demandes':\n",
    "        # Délais négatifs ou >100 jours = incohérents\n",
    "        if 'delai_effectif' in df.columns:\n",
    "            df = df[(df['delai_effectif'] > 0) & (df['delai_effectif'] <= 100)]\n",
    "            print(\"✅ Suppression délais incohérents (>100j ou <0)\")\n",
    "\n",
    "        # Âge entre 0 et 120\n",
    "        if 'age_demandeur' in df.columns:\n",
    "            df = df[(df['age_demandeur'] >= 0) & (df['age_demandeur'] <= 120)]\n",
    "            print(\"✅ Filtrage âge plausible\")\n",
    "\n",
    "    elif name == 'centres':\n",
    "        # Capacité : winsorisation des outliers\n",
    "        if 'capacite_jour' in df.columns:\n",
    "            lb, ub = detect_outliers_iqr(df, 'capacite_jour')\n",
    "            df['capacite_jour'] = np.clip(df['capacite_jour'], lb, ub)\n",
    "            print(\"✅ Winsorisation capacité (IQR)\")\n",
    "\n",
    "    elif name == 'communes':\n",
    "        # Densité population >0\n",
    "        if 'densite_pop' in df.columns:\n",
    "            df = df[df['densite_pop'] > 0]\n",
    "            print(\"✅ Suppression densités négatives\")\n",
    "\n",
    "    print(f\"Dimensions après correction : {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Appliquer à tous\n",
    "for name in datasets:\n",
    "    datasets[name] = correct_inconsistencies(datasets[name], name)\n",
    "\n",
    "print(\"\\n✅ Corrections incohérences terminées.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dd7923",
   "metadata": {},
   "source": [
    "## 4. Harmonisation des Formats\n",
    "\n",
    "**Stratégie** :\n",
    "- Dates : Conversion en datetime, format uniforme.\n",
    "- Géographie : Normalisation noms (minuscules, accents).\n",
    "- Catégories : Mapping uniforme (ex. : 'LOME' → 'Lomé')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e2acfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour harmoniser formats\n",
    "def harmonize_formats(df, name):\n",
    "    print(f\"\\n=== Harmonisation formats pour {name} ===\")\n",
    "\n",
    "    # Dates\n",
    "    date_cols = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "    for col in date_cols:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "            print(f\"✅ Conversion datetime pour {col}\")\n",
    "        except:\n",
    "            print(f\"⚠️ Échec conversion {col}\")\n",
    "\n",
    "    # Géographie : normalisation noms\n",
    "    geo_cols = [col for col in df.columns if 'commune' in col.lower() or 'region' in col.lower() or 'ville' in col.lower()]\n",
    "    for col in geo_cols:\n",
    "        df[col] = df[col].astype(str).str.lower().str.strip()\n",
    "        # Mapping spécifique (exemple)\n",
    "        mapping = {'lome': 'lomé', 'kara': 'kara', 'sokode': 'sokodé'}\n",
    "        df[col] = df[col].replace(mapping)\n",
    "        print(f\"✅ Normalisation géo pour {col}\")\n",
    "\n",
    "    # Catégories : uniformisation\n",
    "    cat_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].str.lower().str.strip()\n",
    "        print(f\"✅ Uniformisation catégories pour {col}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "# Appliquer\n",
    "for name in datasets:\n",
    "    datasets[name] = harmonize_formats(datasets[name], name)\n",
    "\n",
    "print(\"\\n✅ Harmonisation formats terminée.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f28a37",
   "metadata": {},
   "source": [
    "## 5. Documentation des Transformations Appliquées\n",
    "\n",
    "**Règles appliquées** :\n",
    "- **Valeurs manquantes** : Imputation médiane pour variables numériques asymétriques (délais), mode pour catégories, suppression si <1% ou critiques (ID).\n",
    "- **Incohérences** : Suppression valeurs hors plages logiques (délais >100j, âge >120), winsorisation IQR pour outliers capacitaires.\n",
    "- **Formats** : Conversion datetime pour colonnes temporelles, normalisation minuscules/accents pour géo, uniformisation catégories.\n",
    "\n",
    "**Justifications** :\n",
    "- Basé sur EDA : Médiane préserve distribution, suppression minimise perte de données.\n",
    "- Cohérence métier : Plages plausibles pour éviter biais.\n",
    "\n",
    "**Impact** : Réduction NaN de X% à Y%, correction Z outliers, harmonisation W colonnes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6db876",
   "metadata": {},
   "source": [
    "## 6. Sauvegarde du Dataset Final\n",
    "\n",
    "**Dataset final** : Jointure unifiée des datasets nettoyés pour analyse/KPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1152a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jointure finale (exemple basé sur EDA)\n",
    "# Adapter selon clés réelles (ex. : region, commune)\n",
    "merged = datasets['demandes'].merge(datasets['centres'], on='region', how='left') \\\n",
    "                             .merge(datasets['communes'], on='commune', how='left') \\\n",
    "                             .merge(datasets['socioeco'], on='region', how='left')\n",
    "\n",
    "print(f\"Dataset final : {merged.shape[0]} lignes, {merged.shape[1]} colonnes\")\n",
    "\n",
    "# Sauvegarde\n",
    "merged.to_csv('../data/dataset_nettoye.csv', index=False, encoding='utf-8')\n",
    "print(\"✅ Dataset sauvegardé : ../data/dataset_nettoye.csv\")\n",
    "\n",
    "# Aperçu\n",
    "print(merged.head())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
